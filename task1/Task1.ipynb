{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c550453c-7510-4b25-98ba-9c16598372d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import random\n",
    "from base64 import b64encode\n",
    "import numpy\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ee0bd-538e-4f93-84fd-2f59f262bdf5",
   "metadata": {},
   "source": [
    "## Initialize Component\n",
    "\n",
    "We first initialize components from CLIP Model and Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "45ef33d6-8c8b-4412-8ee1-97813e7fc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space.\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e375fcd-38cb-4879-a8a8-d9ed56e8b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale factor that diffusion model uses\n",
    "DIFFUSION_SCALE_FACTOR = 0.18215"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4006ff1-1d26-47f3-9dd1-92154486d8f3",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5b359071-e513-4e5e-b48d-524850468b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
    "    return DIFFUSION_SCALE_FACTOR * latent.latent_dist.sample()\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / DIFFUSION_SCALE_FACTOR) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d478ba-8c9e-4d02-bc8c-808edce72b09",
   "metadata": {},
   "source": [
    "### Noise Latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb19ba-412e-4fec-a4e5-5906236cb5e3",
   "metadata": {},
   "source": [
    "Based on the timestep index $t$, we retrieve a sigma and then sample a random noise. The noise is added to the latent at timestep $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f670e30a-e7ad-4a2d-b99b-b6d590fb7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_latent(latent, timestep_idx):\n",
    "    # Get the sigma value for the randomly selected timestep\n",
    "    start_sigma = scheduler.sigmas[timestep_idx]\n",
    "\n",
    "    # Generate random noise with the same shape as the latent\n",
    "    noise = torch.randn_like(latent)\n",
    "\n",
    "    timestep = scheduler.timesteps[timestep_idx]\n",
    "\n",
    "    # Add noise to the latent using the scheduler at the randomly selected timestep\n",
    "    noised_latent = scheduler.add_noise(latent, noise, timesteps=torch.tensor([timestep]))\n",
    "\n",
    "    return noised_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac4623-5ace-42d3-911a-2624233e5a2a",
   "metadata": {},
   "source": [
    "### Denoise Latent\n",
    "\n",
    "After we noise a latent on timestep $t$, we then try to denoise the image.\n",
    "We first predict the noise added using the `unet` along with textual embeddings\n",
    "The latent is then denoised conditioned on textual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96e9179b-ff1b-428f-bdee-9cb5d62fe5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 7.5\n",
    "\n",
    "def denoise_latent_conditioned(latent, timestep_idx, text_embeddings):\n",
    "    timestep_idx = min(timestep_idx, len(scheduler.timesteps) - 2)\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latent] * 2)\n",
    "    sigma = scheduler.sigmas[timestep_idx]\n",
    "    timestep = scheduler.timesteps[timestep_idx]\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, timestep, latent).prev_sample\n",
    "\n",
    "    return latents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cc6de-0d89-4601-92bf-d9f8c9eb946b",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "We generate text embeddings by first tokening the prompt/classname using tokenizer from CLIP and then encode it using CLIP Text Encoder.\n",
    "We also pad it with unconditional embeddings for guidance free classiciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f0e08def-dee9-47b1-af66-00c4784915e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "def genenerate_text_embeddings(prompt: str):\n",
    "    # Prep text (same as before)\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d64710a5-30a9-4531-a8b5-40f4a36959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_t := exp(âˆ’7t)\n",
    "def timestep_weight(t: int):\n",
    "    return e ** (-7*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "82c2d9a4-1868-4c6a-8597-b5ea242b307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scores = 20\n",
    "max_scores = 2000\n",
    "cutoff_pval = 2 * e**-3\n",
    "sd_model = 'CompVis/stable-diffusion-v1-4'\n",
    "\n",
    "# weight function used\n",
    "# score = w_t * l2_loss\n",
    "\n",
    "def diffuser_classifier(image_path: str, classes: list[str]):\n",
    "    k = len(classes)\n",
    "\n",
    "    if k == 0:\n",
    "        return []\n",
    "    if k == 1:\n",
    "        return [1.0]\n",
    "        \n",
    "    scores = {y_i: [] for y_i in classes}\n",
    "    for i in tqdm(range(max_scores)):\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image_latent = pil_to_latent(image)\n",
    "\n",
    "        \"\"\"\n",
    "        Noise the Image\n",
    "        \"\"\"\n",
    "        # t ~ U([0, 1]) - t can take any value from 0 to 1 and each value is equally likely\n",
    "        t = random.uniform(0, 1)\n",
    "        num_steps = len(scheduler.timesteps)\n",
    "        timestep_idx = int(t * (num_steps - 1))\n",
    "\n",
    "        # x_t ~ q(x_t|x)\n",
    "        x_t = noise_latent(\n",
    "            latent=image_latent,\n",
    "            timestep_idx=timestep_idx,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Score against remaining classes\n",
    "        \"\"\"\n",
    "        for y_i in scores:\n",
    "            embeddings = genenerate_text_embeddings(y_i)\n",
    "            \n",
    "            denoised_latent = denoise_latent_conditioned(\n",
    "                latent=x_t,\n",
    "                timestep_idx=timestep_idx,\n",
    "                text_embeddings=embeddings,\n",
    "            )\n",
    "\n",
    "            # calculate L2 loss\n",
    "            squared_loss = torch.mean((image_latent - denoised_latent) ** 2)\n",
    "            w_t = timestep_weight(t)\n",
    "            weighted_loss = w_t * squared_loss\n",
    "            \n",
    "            scores[y_i].append(weighted_loss)\n",
    "\n",
    "    means = {key: torch.mean(torch.tensor(scores[key])).item() for key in scores}\n",
    "    min_class = min(means, key=means.get)\n",
    "\n",
    "    print(f\"label: {y_i}, t:{t}, loss:\", means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b3406-7073-4973-9e1a-af998d0414b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser_classifier(\n",
    "    image_path='bear.png', \n",
    "    classes=['apple', 'god', 'cat', 'bear', 'duck']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbabe425-843d-41f0-a55c-d46a639ac5d9",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba04d7-f238-44db-a749-e52d0785fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16, vit_b_16\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13e0e6-63e5-48bc-9ba0-9aaced701962",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592934dd-d17a-41f4-9219-876e5f7a0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "cifar10_train = datasets.CIFAR10(root='./datasets/base', train=True, download=True, transform=None)\n",
    "cifar10_test = datasets.CIFAR10(root='./dataset/base', train=False, download=True, transform=None)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_loader = DataLoader(cifar10_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467c31f-95cd-4aae-84ac-85d06ed8365f",
   "metadata": {},
   "source": [
    "## Extract Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b459c-8784-41d5-9a22-a2cc44cff659",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_threshold = 10\n",
    "high_threshold = 200\n",
    "\n",
    "def extract_shape(image):\n",
    "    image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    canny_image = Image.fromarray(image)\n",
    "    return canny_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a551280-3f54-40f1-9726-374dfe94a6a2",
   "metadata": {},
   "source": [
    "## Extract Texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d8d2f-4b67-4536-be66-5fb492a64dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texture(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (11, 11), 0)\n",
    "    image = blurred_image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    canny_image = Image.fromarray(image)\n",
    "    return canny_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3e745-bcfc-49e7-8813-ec09b8674365",
   "metadata": {},
   "source": [
    "## Extract Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e7f97-2ee6-4857-b494-028a9dc07f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = blurred_image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    canny_image = Image.fromarray(image)\n",
    "    return canny_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bded3a-7ef3-4931-b2f5-5e32bf1680b4",
   "metadata": {},
   "source": [
    "## Evaluate VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75eea7-5366-49f8-9b4b-9a23793bacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = vgg16(pretrained=True)\n",
    "vgg16_model = vgg16_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27315054-1b8a-4d0a-836f-5d0b2473b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # resize to default vgg16 input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root='./dataset/base', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,shuffle=False)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device='cuda'\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            if i < 10:\n",
    "                i += 1\n",
    "                print(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            # print(f\"{correct}/{total}\")\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = evaluate_model(vgg16_model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc33dac-66a0-45bc-a65e-9bd45ca4e2aa",
   "metadata": {},
   "source": [
    "## Load ViT Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64615de9-ad20-4fad-b491-16f09d39f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = vit_b_16(pretrained=True)\n",
    "vit_model = vit_model.to('cuda')\n",
    "\n",
    "vgg16_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # resize to default vgg16 input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root='./dataset/base', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,shuffle=False)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device='cuda'\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            if i < 10:\n",
    "                i += 1\n",
    "                print(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            # print(f\"{correct}/{total}\")\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = evaluate_model(vit_model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d172fe2-8310-4f13-92c1-aece021f2a3e",
   "metadata": {},
   "source": [
    "## Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba692fa-1bfa-4d71-8ede-cd2d8e869ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "vit_model = vit_b_16(pretrained=True)\n",
    "vit_model = vit_model.to('cuda')\n",
    "\n",
    "vgg16_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # resize to default vgg16 input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(root='./dataset/base', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,shuffle=False)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device='cuda'\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch in tqdm(test_loader):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            if i < 10:\n",
    "                i += 1\n",
    "                print(outputs)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            # print(f\"{correct}/{total}\")\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test dataset\n",
    "accuracy = evaluate_model(clip_model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
